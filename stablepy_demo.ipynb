{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTtwAB3UX9r02s0CiGOolX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stablepy"
      ],
      "metadata": {
        "id": "BQeX1ykNNMla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "TYqyA785NZF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itGk9x06NL5O"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/R3gm/stablepy.git@v0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download our models and other stuffs"
      ],
      "metadata": {
        "id": "KdNshU7kNbLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "!wget https://huggingface.co/frankjoshua/toonyou_beta6/resolve/main/toonyou_beta6.safetensors\n",
        "\n",
        "# VAE\n",
        "!wget https://huggingface.co/fp16-guy/anything_kl-f8-anime2_vae-ft-mse-840000-ema-pruned_blessed_clearvae_fp16_cleaned/resolve/main/anything_fp16.safetensors\n",
        "\n",
        "# LoRAs\n",
        "!wget https://civitai.com/api/download/models/183149 --content-disposition\n",
        "!wget https://civitai.com/api/download/models/97655 --content-disposition\n",
        "\n",
        "# Embeddings\n",
        "!wget https://huggingface.co/embed/negative/resolve/main/bad-hands-5.pt\n",
        "!wget https://huggingface.co/embed/negative/resolve/main/bad-artist.pt\n",
        "\n",
        "# Upscaler\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth"
      ],
      "metadata": {
        "id": "GtTdza7SNexT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Stable diffusion 1.5"
      ],
      "metadata": {
        "id": "amYJfvMwOKnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we pass the path of the model we will use.\n",
        "\n",
        "The default task is txt2img but it can be changed to: openpose, canny, mlsd, scribble, softedge, segmentation, depth, normalbae, lineart, shuffle, ip2p or inpaint"
      ],
      "metadata": {
        "id": "PvTT0AiKRX0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stablepy import Model_Diffusers\n",
        "import torch\n",
        "\n",
        "model_path = \"./toonyou_beta6.safetensors\"\n",
        "vae_path = \"./anything_fp16.safetensors\"\n",
        "\n",
        "model = Model_Diffusers(\n",
        "    base_model_id = model_path, # path to the model\n",
        "    task_name = \"canny\", # task\n",
        "    vae_model = vae_path, # path vae\n",
        ")"
      ],
      "metadata": {
        "id": "9hvZngGDOhh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To switch tasks, we can call `model.load_pipe()` and specify the new task or model. This will load the necessary components."
      ],
      "metadata": {
        "id": "CQNbptvYSs4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_pipe(\n",
        "    base_model_id = model_path, # path to the model\n",
        "    task_name = \"txt2img\", # task\n",
        "    vae_model = None, # Use default VAE\n",
        ")"
      ],
      "metadata": {
        "id": "jnO6p80qPRS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a basic txt2img task in which we can specify different common parameters, such as Loras, embeddings, upscaler, etc."
      ],
      "metadata": {
        "id": "fqmU2o0fTUvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "lora1_path = \"./EmptyEyes_Diffuser_v10.safetensors\"\n",
        "lora2_path = \"./FE_V2.safetensors\" # pixel art lora\n",
        "upscaler_path = \"./RealESRGAN_x4plus.pth\"\n",
        "\n",
        "images, image_list = model(\n",
        "    prompt = \"pixel art (masterpiece, best quality), 1girl, collarbone, wavy hair, looking at viewer, blurry foreground, upper body, necklace, contemporary, plain pants, ((intricate, print, pattern)), ponytail, freckles, red hair, dappled sunlight, smile, happy,\",\n",
        "    negative_prompt = \"(worst quality, low quality, letterboxed), bad_artist_token, bad_hand_token\",\n",
        "    img_width = 512,\n",
        "    img_height = 1024,\n",
        "    num_images = 1,\n",
        "    num_steps = 30,\n",
        "    guidance_scale = 8.0,\n",
        "    clip_skip = True,\n",
        "    seed = -1, # random seed\n",
        "    sampler=\"DPM++ SDE Karras\",\n",
        "    syntax_weights=\"Compel\",\n",
        "\n",
        "    lora_A = lora1_path,\n",
        "    lora_scale_A = 0.8,\n",
        "    lora_B = lora2_path,\n",
        "    lora_scale_B = 0.9,\n",
        "\n",
        "    textual_inversion=[(\"bad_artist_token\", \"./bad-artist.pt\"), (\"bad_hand_token\", \"./bad-hands-5.pt\")], # Is a list of tuples with [(\"<token_activation>\",\"<path_embeding>\"),...]\n",
        "\n",
        "    upscaler_model_path = upscaler_path, # High res fix\n",
        "    upscaler_increases_size=1.5,\n",
        ")\n",
        "\n",
        "for image in images:\n",
        "  display(image)"
      ],
      "metadata": {
        "id": "zVrtRLuFPSiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ControlNet"
      ],
      "metadata": {
        "id": "zSYYzJ7FXO2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_pipe(\n",
        "    base_model_id = model_path,\n",
        "    task_name = \"canny\",\n",
        "    # Use default VAE\n",
        ")"
      ],
      "metadata": {
        "id": "4BsMsk0SXjtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select a control image"
      ],
      "metadata": {
        "id": "PF1PCOyKXvSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "!wget https://huggingface.co/lllyasviel/sd-controlnet-canny/resolve/main/images/bird.png\n",
        "\n",
        "control_image = \"bird.png\"\n",
        "image = Image.open(control_image)\n",
        "display(image)"
      ],
      "metadata": {
        "id": "ia5fu84QW1MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference with canny"
      ],
      "metadata": {
        "id": "3AjvhCa3dsBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, image_list = model(\n",
        "    prompt = \"(masterpiece, best quality), bird\",\n",
        "    negative_prompt = \"(worst quality, low quality, letterboxed)\",\n",
        "    image = control_image,\n",
        "    # preprocessor_name=\"None\", only canny not need the preprocessor_name, active by default\n",
        "    preprocess_resolution = 512, # It is the resize of the image that will be obtained from the preprocessor.\n",
        "    image_resolution = 768, # The equivalent resolution to be used for inference.\n",
        "    controlnet_conditioning_scale = 1.0, # ControlNet Output Scaling in UNet\n",
        "    control_guidance_start = 0.0, # ControlNet Start Threshold (%)\n",
        "    control_guidance_end= 1.0, # ControlNet Stop Threshold (%)\n",
        "\n",
        "    upscaler_model_path = upscaler_path,\n",
        "    upscaler_increases_size=1.5,\n",
        ")\n",
        "\n",
        "for image in images:\n",
        "  display(image)"
      ],
      "metadata": {
        "id": "NefvFTvCWL8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "valid `preprocessor_name` depending on the task: HED, HED safe, Midas, MLSD, Openpose, PidiNet, PidiNet safe, NormalBae, Lineart, Lineart coarse, LineartAnime, None (anime), ContentShuffle, DPT or UPerNet"
      ],
      "metadata": {
        "id": "_4ETyFwVm4wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adetailer"
      ],
      "metadata": {
        "id": "fcuPwmjudAc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_pipe(\n",
        "    base_model_id = model_path,\n",
        "    task_name = \"txt2img\",\n",
        ")"
      ],
      "metadata": {
        "id": "o_0OE-PaPGOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There must be a match of parameters for good results to be obtained with adetailer, it is also useful to use `strength` in adetailer_inpaint_params with low values ​​below 0.4."
      ],
      "metadata": {
        "id": "RxBK7tcLdENa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = 512\n",
        "img_height = 1024\n",
        "num_steps = 30\n",
        "CFG = 8.0\n",
        "\n",
        "adetailer_inpaint_params = {\n",
        "    \"prompt\": None, # main prompt will be use\n",
        "    \"negative_prompt\" : None,\n",
        "    \"strength\" : 0.35, # need low values\n",
        "    \"num_inference_steps\": num_steps,\n",
        "    \"width\": img_width,\n",
        "    \"height\": img_height,\n",
        "    \"guidance_scale\" : CFG,\n",
        "}\n",
        "\n",
        "adetailer_params = {\n",
        "    \"face_detector_ad\" : True,\n",
        "    \"person_detector_ad\" : True,\n",
        "    \"hand_detector_ad\" : False,\n",
        "    \"inpaint_only\" : adetailer_inpaint_params,\n",
        "    \"mask_dilation\" : 4,\n",
        "    \"mask_blur\" : 4,\n",
        "    \"mask_padding\" : 32,\n",
        "}\n",
        "\n",
        "images, image_list = model(\n",
        "    prompt = \"(masterpiece, best quality), 1girl, collarbone, wavy hair, looking at viewer, blurry foreground, upper body, necklace, contemporary, plain pants, ((intricate, print, pattern)), ponytail, freckles, red hair, dappled sunlight, smile, happy,\",\n",
        "    negative_prompt = \"(worst quality, low quality, letterboxed)\",\n",
        "    img_width = img_width,\n",
        "    img_height = img_height,\n",
        "    num_images = 1,\n",
        "    num_steps = num_steps,\n",
        "    guidance_scale = CFG,\n",
        "    clip_skip = True,\n",
        "    seed = 33,\n",
        "    sampler=\"DPM++ SDE Karras\",\n",
        "\n",
        "    FreeU=True, # Improves diffusion model sample quality at no costs.\n",
        "    adetailer_active=True,\n",
        "    adetailer_params=adetailer_params,\n",
        "\n",
        "    # upscaler_model_path = upscaler_path,\n",
        "    # upscaler_increases_size=1.5,\n",
        ")\n",
        "\n",
        "for image in images:\n",
        "  display(image)"
      ],
      "metadata": {
        "id": "F_b2jCSBcVBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For more details about the parameters\n",
        "help(model.__call__)"
      ],
      "metadata": {
        "id": "iMxDxgJIVoQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}